
from langchain_community.document_loaders import PyPDFLoader # Importa a classe para carregar documentos PDF (parte do LangChain)
from langgraph.graph import StateGraph , START, END # Importa as classes principais para construir o grafo de estado e os n√≥s de in√≠cio/fim
from langgraph.graph.message import add_messages # Importa o utilit√°rio para adicionar mensagens ao hist√≥rico de forma segura no LangGraph
from langchain_core.messages import SystemMessage, AIMessage, HumanMessage, BaseMessage # Importa os tipos de mensagens (Sistema, AI, Humano, Base)
from typing import TypedDict, Annotated, Sequence, List, Union # Importa tipagens do Python para definir estruturas de dados e anota√ß√µes
from langchain_core.tools import tool # Importa o decorador 'tool' do LangChain para criar fun√ß√µes que o LLM pode chamar
from langchain_core.tools import BaseTool # Importa a classe base para tipagem de ferramentas
import os # Importa o m√≥dulo para interagir com o sistema operacional (caminhos de arquivo)
import json # Importa o m√≥dulo JSON para serializa√ß√£o e desserializa√ß√£o (essencial para tratar argumentos das Tools)

from langchain_core.messages import ToolMessage

# Para usar o modelo OpenAI via LangChain
from langchain_groq import ChatGroq # Importa a classe espec√≠fica para usar o Groq via LangChain
from dotenv import load_dotenv # Importa para carregar vari√°veis de ambiente (GROQ_API_KEY)

load_dotenv() # Carrega as vari√°veis de ambiente do arquivo .env


PASTA_CURRICULOS = 'C:\\Users\\Windows\\Documents\\pasta_curriculos' 


# DEFINI√á√ÉO DE ESTADO DO AGENTE (TypedDict)

class AgentState(TypedDict):
    """
    Define a estrutura de dados (estado) que ser√° passada entre os n√≥s do grafo.
    """
    # Define o hist√≥rico de mensagens. 'add_messages' garante que novas mensagens sejam anexadas.
    # Annotated[..., operator.add] √© a forma do LangGraph de dizer: combine o novo valor com o anterior.
    messages: Annotated[Sequence[BaseMessage], add_messages]
    # Lista para armazenar o conte√∫do de texto dos curr√≠culos.
    carregar: List[str] 
    # Flag booleana para indicar se a ferramenta 'carregar_pdf' j√° foi executada.
    data_loaded: bool 


# FERRAMENTA (TOOL)

@tool
def carregar_pdf(path: str = PASTA_CURRICULOS) -> str:
    """Carrega todos os textos de curr√≠culos em formato PDF de uma pasta espec√≠fica."""
    try:
        # Lista todos os arquivos na pasta que terminam com ".pdf"
        
        file_pdf = [os.path.join(path, f) for f in os.listdir(path) if f.endswith(".pdf")]
        textos_curriculos = [] # Lista para armazenar o conte√∫do de texto dos PDFs
        
        # Verifica se encontrou algum arquivo PDF
        if not file_pdf:
            return f"‚ùå Erro: Nenhum arquivo PDF encontrado na pasta: {path}"

        # Itera sobre cada arquivo PDF encontrado
        for file in file_pdf:
            loader = PyPDFLoader(file) # Inicializa o carregador de PDF
            pages = loader.load() # Carrega o conte√∫do do PDF, retornando uma lista de Documentos
            # Concatena o conte√∫do de todas as p√°ginas em uma √∫nica string por curr√≠culo
            texto_curriculo = "\n".join([p.page_content for p in pages])
            textos_curriculos.append(texto_curriculo) # Adiciona o texto do curr√≠culo √† lista
            
        # Definindo o separador claro entre os curr√≠culos para o LLM
        separator = "\n--- NOVO CURR√çCULO ---\n"
        # Convertendo a lista de curr√≠culos em uma √∫nica string
        full_text = separator.join(textos_curriculos)
        
        # Retorna a string de observa√ß√£o que ser√° passada ao LLM
        return f"‚úÖ Documentos carregados com sucesso. Total de {len(textos_curriculos)} curr√≠culos. Conte√∫do a ser analisado: \n{full_text}"

    except FileNotFoundError:
        return f"‚ùå Erro: O caminho da pasta '{path}' n√£o foi encontrado. Verifique o caminho."
    except Exception as e:
        return f"‚ùå Erro ao carregar PDFs: {e}"


# N√ì: LLM AGENTE (Decisor e Analisador)

def executar_llm(state: AgentState) -> AgentState:
    """
    N√≥ principal: Chama o LLM para analisar, responder ou gerar uma Tool Call.
    """
    model_name = "openai/gpt-oss-20b" # Define o nome do modelo Groq
    groq_api_key = os.getenv("GROQ_API_KEY")
    
    # Inicializa o LLM e liga a ferramenta 'carregar_pdf' a ele
    
    llm = ChatGroq(model=model_name, groq_api_key=groq_api_key, temperature=0.3).bind_tools([carregar_pdf]) 

    # Mensagem de sistema que define o papel e a tarefa do LLM
    SYSTEM_MESSAGE = SystemMessage(
        "Voc√™ √© um especialista em an√°lise de curr√≠culos e um assistente de recrutamento. "
        "Seu trabalho √© analisar os curr√≠culos fornecidos, um por um. Estamos em busca de um programador "
        "com base s√≥lida em LLMs e Python. "
        "Para cada curr√≠culo, forne√ßa uma an√°lise detalhada, incluindo:\n"
        "1. Pontos Fortes (ligados a LLMs/Python).\n"
        "2. Pontos Fracos/√Åreas de Oportunidade.\n"
        "3. Recomenda√ß√£o de Contrata√ß√£o (Sim/N√£o/Talvez).\n"
        "Use a ferramenta 'carregar_pdf' apenas se a √∫ltima mensagem do usu√°rio indicar que √© hora de carregar/analisar os dados."
    )
    
    
    messages_to_llm = [SYSTEM_MESSAGE] + state["messages"] 
    
    llm_result = llm.invoke(messages_to_llm) # Invoca o LLM com o hist√≥rico e as ferramentas
    
    # Retornando o estado atualizado com a nova mensagem gerada pelo LLM.
    return {
        "messages": [llm_result] # Adicionando a resposta (ou a Tool Call) do LLM ao hist√≥rico
    }


# N√ì: EXECUTOR DE FERRAMENTAS (TOOL CALLING)

def executa_tool(state: AgentState) -> AgentState:
    """
    N√≥ que executa a Tool Call (chamada de fun√ß√£o) gerada pelo LLM e devolve o resultado.
    """
    tool_call_message = state["messages"][-1] # Pega a √∫ltima mensagem (deve ser a Tool Call do LLM)
    tool_calls = tool_call_message.additional_kwargs.get("tool_calls", []) # Extrai as chamadas de ferramenta
    
    if not tool_calls:
        return state 

    tool_call = tool_calls[0] # Pega a primeira Tool Call
    func_name = tool_call["function"]["name"] # Nome da fun√ß√£o a ser executada
    raw_args = tool_call["function"]["arguments"] # Argumentos da fun√ß√£o (pode ser string JSON ou dict)
    tool_call_id = tool_call["id"] # ID da chamada 

    
    try:
        if isinstance(raw_args, str):
            
            func_args = json.loads(raw_args)
        else:
            
            func_args = raw_args
            
    except json.JSONDecodeError:
        
        return {"messages": [AIMessage(
            content="Erro na Tool: O LLM gerou argumentos JSON inv√°lidos para a fun√ß√£o.", 
        )]}
        
    
    if func_name == carregar_pdf.name:
        # Executa a tool: **func_args desempacota o dicion√°rio de argumentos para a fun√ß√£o.
        observation = carregar_pdf.invoke(func_args) 
        
        # Criando a mensagem de observa√ß√£o para ser devolvida ao LLM
        tool_messages_to_add = [
            ToolMessage( # Usa ToolMessage
                content=observation, 
                name=func_name, 
                tool_call_id=tool_call_id # Liga a observa√ß√£o √† chamada original do LLM
            )
        ]
        
        # Retorna a atualiza√ß√£o de estado: adiciona a ToolMessage e atualiza a flag
        return {
            "messages": tool_messages_to_add,
            "data_loaded": True
        }
    
    return state # Retorna o estado se a tool n√£o for reconhecida


# ROTEAMENTO CONDICIONAL

def roteador(state: AgentState) -> str:
    """
    Fun√ß√£o de roteamento: Decide o pr√≥ximo passo no grafo com base na sa√≠da do LLM.
    """
    last_message = state["messages"][-1] # Pega a √∫ltima mensagem gerada pelo n√≥ 'executar_llm'
    
    # Verificando se a √∫ltima mensagem cont√©m uma chamada de ferramenta
    if last_message.additional_kwargs.get("tool_calls"):
        return "tool_node" # Se sim, vai para o n√≥ que executa a ferramenta
        
    # Se n√£o houver tool call, o LLM gerou a resposta final
    return END # Termina a execu√ß√£o do grafo



# CONSTRU√á√ÉO DO GRAFO (LANGGRAPH)

builder = StateGraph(AgentState) # Criando o objeto construtor do grafo com o tipo de estado definido

# Adicionando os nodes
builder.add_node("executar_llm", executar_llm) # N√≥ principal (LLM)
builder.add_node("tool_node", executa_tool) # N√≥ executor de Tools

# Definindo o ponto de entrada. O processo sempre come√ßa com o LLM para decidir a a√ß√£o.
builder.set_entry_point("executar_llm") 

# Define a transi√ß√£o condicional que parte do LLM
builder.add_conditional_edges(
    "executar_llm", # O n√≥ de origem
    roteador, # A fun√ß√£o que decide o caminho (usa a Tool ou termina)
    {
        "tool_node": "tool_node", # Se o roteador retornar "tool_node", vai para o n√≥ de execu√ß√£o
        END: END# Se retornar END, o grafo termina
    }
)

# Define a aresta de retorno: ap√≥s executar a Tool, o resultado volta para o LLM
# O LLM ler√° a ToolMessage (observa√ß√£o) e gerar√° a resposta final
builder.add_edge("tool_node", "executar_llm") 

# Compilar Grafo
graph = builder.compile() # Compila√ß√£o do grafo
# graph.get_graph().draw_mermaid_png(output_file_path="ark.png") # Comando para gerar o diagrama visual do grafo


# LOOP DE INTERA√á√ÉO (EXECU√á√ÉO)

print("‚úÖ Agente de An√°lise de Curr√≠culos Iniciado.")
print(f"üìÅ Pasta de Curriculos: {PASTA_CURRICULOS}")
print("---")

# Loop de intera√ß√£o com o usu√°rio
while True:
    user_input = input("üë§ Voc√™: ") # Solicita a entrada do usu√°rio
    
    # Verifica a condi√ß√£o de sa√≠da
    if user_input.lower() in ['q', 'sair']: 
        print("At√© mais!")
        break
        
    
    initial_state = {"messages": [HumanMessage(content=user_input)]}

    # Invocando o grafo com o estado inicial
 
    result = graph.invoke(initial_state) 
    
    # Imprime o separador
    print("-" * 20) 
    
    # Imprime a √∫ltima mensagem de conte√∫do gerada pelo LLM (a resposta final)
    final_answer = result['messages'][-1].content
    print(f"ü§ñ AI: {final_answer}")
